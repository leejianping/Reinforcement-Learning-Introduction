
# 第六讲 价值函数的近似法(Value Function Approximation)

## 前言

前面第一部分（前五讲）主要介绍强化学习的基础理论和强化学习算法的核心思想，这些算法的价值函数用表格来存储，因此也称为表格解方法（Tabular Solution methods）。虽然这些算法可以得到准确的解，但是这些算法只能求解规模比较小的问题。因此，在接下来的第二部分，将聚焦于各种价值函数、策略函数的近似表示；用于求解大规模问题，这部分称为近似解方法（Approximation solution methods）。

在很多实际任务中，几乎遇到的每一状态在之前都是没见过的。在这样的状态下做出合理的决策，需要能从之前遇到的且与当前这个状态一定程度上相似的不同状态的经历中总结出来这个合理的决策。换句话说，泛化（generalization）才是最关键的问题。幸运的是，从例子中泛化的方法已经被广泛研究。我们只需要将强化学习与现有的泛化方法相结合。我们所需要的泛化类型通常被称为函数近似（function approximation），因为它从目标函数（例如，值函数）中获取例子，然后尝试从这些例子中泛化到构建一个对整个函数的近似。函数近似是监督学习的一个实例，也是机器学习、人工神经网络、模式识别、统计曲线拟合的主要研究问题。

在本讲中先介绍价值函数的近似表示和学习。下一讲介绍策略的近似表示和学习。在实际应用中，对于大规模问题，状态和行为空间都比较大的情况下，精确获得各种价值函数v(S)和q(s,a)几乎是不可能的。这时候需要找到近似的函数，具体可以使用线性组合、神经网络以及其他方法来近似价值函数。本讲的新颖性在于近似函数不是表示成表格，但是表示成带有权重向量w的参数化函数形式，

$$
\hat{v}(s,w) \approx v_{\pi}(s) 
$$

上式表示状态s给定权重向量时的近似值。如 $\hat{v}$ 可能是状态的特征的线性函数，w 为特征权重向量。更一般地，$\hat{v}$ 可能是多层人工神经网络计算的函数，w为在所有层中的连接权重向量。

通过函数近似，可以用少量的参数w来拟合实际的各种价值函数。本讲同时在理论上比较了各种近似方法的优缺点以及是否收敛于最优解等。本讲将各种近似方法分为两大类，一类是“增量方法”，针对每一步，近似函数得到一些信息，立即优化近似函数，主要用于在线学习；另一类是“批方法”，针对一批历史数据集中进行近似。两类方法没有明显的界限，相互借鉴。

## 6.1 简介

**大规模强化学习(Large-Scale Reinforcement Learning)**

大规模强化学习指用强化学习来解决大规模的问题。如下围棋有 $10^{170}$ 个状态空间。我们该如何才能将前面两讲中的无模型预测和控制的方法规模扩大化呢？

**近似价值函数(Value Function Approximation)**

到目前为止，我们使用查表（Table Lookup）的方式来表示价值函数，因此，每一个状态或者每一个状态行为对与表格中的一个V或Q值的条目对应。对于大规模问题，有太多的状态或动作需要存储在内存中，而且对于每一个状态单独地学习得到价值也是一个很慢的过程。

大规模问题的解决方法是，
1. 用函数近似来估计价值函数：

$$
\hat{v}(s,w) \approx v_{\pi}(s)\\
\hat{q}(s,a,w) \approx q_{\pi}(s,a)  
$$

2. 从经历过的状态泛化到未经历的状态
3. 使用MC或TD学习来更行参数w

**近似函数的类型(Types of Value Function Approximation)**

对于强化学习，根据输入和输出的不同，近似函数分为三类，如图，

![](./images ":size=350")

1. 输入为状态s，输出为状态价值函数的近似
2. 输入为状态动作对s,a，输出为状态动作价值函数的近似
3. 输入为状态s，输出为该状态下所有可能的动作的动作价值函数的近似

**有哪些近似函数呢？**

有非常多的近似函数，如特性的线性组合（Linear combinations of features），神经网络，决策树，最邻近法等等。所有和机器学习相关的一些算法都可以应用到强化学习中来，但是我们主要考虑可求导(differentiable)的近似函数，如线性回归和神经网络在强化学习里应用得比较广泛。

强化学习应用的场景其数据通常是非静态(non-stationary)、非独立同分布(non-iid)的，因为一个状态数据是可能是持续流入的，而且下一个状态通常与前一个状态是高度相关的。因此，我们需要近似函数的方法能够处理非静态、非独立同分布的数据。

接下来将分别从增量方法和批方法两个角度来讲解价值函数的近似方法，其主要思想都是梯度下降，与机器学习中的随机梯度下降和批梯度下降相对应。

## 6.2 增量方法(Incremental Methods)

首先回顾一下梯度、梯度下降即其应用。

### 6.2.1 梯度下降(Gradient Descent)

设J(w)是一个关于参数向量w的可导（可微）函数，定义J(w)的梯度如下：

$$
\nabla_{w} J(w)= \begin{pmatrix} \frac{\partial J(w)}{\partial w_{1}} \\ \vdots \\ \frac{\partial J(w)}{\partial w_{n}} \\ \end{pmatrix}
$$

为了寻找J(w)的局部最小值,朝着负梯度的方向更新梯度向量w,

$$
\Delta w = -\frac{1}{2}\alpha \nabla_{w} J(w)
$$

其中，$\alpha$ 是步长参数，机器学习里称为学习速率。

梯度的示意图如下，

![](./images/1%20gradient.PNG ":size=350")


**用随机梯度下降来求函数的近似**


* 找到参数向量w,使得近似函数 $\hat{v}(s,w)$  与实际函数 $ v_{\pi}(s)$ 的均方差最小：

$$
J(w) = \mathbb E_{\pi}[（v_{\pi}(S)-\hat{v}(S,w))^{2}] 
$$

* 用梯度下降找到局部最小值：
  
$$\begin {aligned}
\Delta w &= -\frac{1}{2}\alpha \nabla_{w} J(w)\\
&= \alpha \mathbb E_{\pi}[(v_{\pi}(S)-\hat{v}(S,w))\nabla_{w}\hat{v}(S,w)]
\end {aligned}
$$

* 随机梯度下降算法会对梯度进行采样：

$$
\Delta w = \alpha (v_{\pi}(S)-\hat{v}(S,w))\nabla_{w}\hat{v}(S,w)
$$

* 期望更新等于全梯度（full gradient）下降更新


### 6.2.2 线性函数近似(Linear Function Approximation)




![](./images ":size=350")

$$\begin {aligned}

\end {aligned}
$$

$$$$

### 6.2.3 增量预测算法(Incremental Prediction Algorithms)



### 6.2.4 增量控制算法(Incremental control Algorithms)



### 6.2.5 收敛性(Convergence)



## 6.3 批方法(Batch Methods)


### 6.3.1 最小二乘预测(Least square prediction)


### 6.3.2 最小二乘控制(Least square control)

小结：
本节先讲解了引入价值函数的近似表示的重要性，接着从梯度开始讲起，使用梯度下降可以找到一个目标函数的极小值，以此设计一个目标函数来寻找近似价值函数的参数。有机器学习基础的读者理解本节会非常容易。本节的理论重点在于理解不同强化学习方法在应用不同类型的近似函数时的收敛性，能否获得全局最优解，以及DQN算法的设计思想及算法流程。本讲罗列了大量的数学公式，并以线性近似函数为例给出了具体的参数更新办法，这些公式在强大的机器学习库面前已经显得有些过时了，但对于理解一些理论还是有些帮助的。此外，在本讲的最后还简单介绍了不需要迭代更新直接求解线性近似函数参数的方法，可以作为补充了解。一个令人吃惊地结论是，所有函数近似的理论结果同样可以用于部分观测的情况。因为，参数化函数形式不能从状态的某些方面来估计，这就像状态的这些方面不可观测。


参考
1.David Silver第5课
2.Richard Sutton 《Reinforcement Learning  A Introduction》chapter(6，7，12)
3.叶强《David Silver强化学习公开课中文讲解及实践》第五讲
4.搬砖的旺财《David Silver 增强学习——Lecture 5 不基于模型的控制（三）》


![](./images ":size=350")

$$\begin {aligned}

\end {aligned}
$$

$$$$